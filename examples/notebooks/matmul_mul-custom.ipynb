{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "\n",
    "from torch_mlir.dialects.torch.importer.jit_ir import ClassAnnotator, ModuleBuilder\n",
    "from torch_mlir.dialects.torch.importer.jit_ir.torchscript_annotations import extract_annotations\n",
    "from torch_mlir_e2e_test.torchscript.annotations import annotate_args, export\n",
    "\n",
    "from torch_mlir.passmanager import PassManager\n",
    "from air.backend import linalg_on_tensors as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = [128,128]\n",
    "\n",
    "class MMult_Mult(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @export\n",
    "    @annotate_args([\n",
    "        None,\n",
    "        (SIZE, torch.int32, True),\n",
    "        (SIZE, torch.int32, True),\n",
    "        (SIZE, torch.int32, True)\n",
    "    ])\n",
    "    def forward(self, a, b, c):\n",
    "        x = torch.mm(b,c)\n",
    "        y = a*x\n",
    "        return y\n",
    "\n",
    "program = MMult_Mult()\n",
    "scripted = torch.jit.script(program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#map = affine_map<(d0, d1) -> (d0, d1)>\n",
      "module attributes {torch.debug_module_name = \"MMult_Mult\"} {\n",
      "  func @forward(%arg0: tensor<128x128xi32>, %arg1: tensor<128x128xi32>, %arg2: tensor<128x128xi32>) -> tensor<?x?xi32> {\n",
      "    %c0_i32 = arith.constant 0 : i32\n",
      "    %0 = linalg.init_tensor [128, 128] : tensor<128x128xi32>\n",
      "    %1 = linalg.fill(%c0_i32, %0) : i32, tensor<128x128xi32> -> tensor<128x128xi32> \n",
      "    %2 = linalg.matmul ins(%arg1, %arg2 : tensor<128x128xi32>, tensor<128x128xi32>) outs(%1 : tensor<128x128xi32>) -> tensor<128x128xi32>\n",
      "    %3 = linalg.generic {indexing_maps = [#map, #map, #map], iterator_types = [\"parallel\", \"parallel\"]} ins(%arg0, %2 : tensor<128x128xi32>, tensor<128x128xi32>) outs(%0 : tensor<128x128xi32>) {\n",
      "    ^bb0(%arg3: i32, %arg4: i32, %arg5: i32):\n",
      "      %5 = arith.muli %arg3, %arg4 : i32\n",
      "      linalg.yield %5 : i32\n",
      "    } -> tensor<128x128xi32>\n",
      "    %4 = tensor.cast %3 : tensor<128x128xi32> to tensor<?x?xi32>\n",
      "    return %4 : tensor<?x?xi32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_annotator = ClassAnnotator()\n",
    "extract_annotations(program, scripted, class_annotator)\n",
    "\n",
    "mb = ModuleBuilder()\n",
    "mb.import_module(scripted._c, class_annotator)\n",
    "\n",
    "pm = PassManager.parse('torchscript-module-to-torch-backend-pipeline,torch-backend-to-linalg-on-tensors-backend-pipeline', mb.module.context)\n",
    "pm.run(mb.module)\n",
    "print(mb.module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbackend = backend.LinalgOnTensorsAirBackend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch_mlir.ir\n",
    "\n",
    "import air.mlir.ir\n",
    "import air.mlir.passmanager\n",
    "import air.compiler.aircc.main as aircc\n",
    "\n",
    "def compile(imported_module: torch_mlir.ir.Module):\n",
    "    with air.mlir.ir.Context():\n",
    "        air_module = air.mlir.ir.Module.parse(str(imported_module))\n",
    "        \n",
    "        # bufferize the linalg dialect\n",
    "        pm = air.mlir.passmanager.PassManager.parse(air.compiler.util.LINALG_TENSOR_TO_MEMREF_PIPELINE)\n",
    "        pm.run(air_module)\n",
    "        \n",
    "        # convert linalg dialect to air dialect\n",
    "        #LINALG_MEMREF_TO_AIR_PIPELINE = \",\".join([\n",
    "        #    \"air-linalg-codegen\",\n",
    "        #    \"canonicalize\",\n",
    "        #    \"cse\",\n",
    "        #    \"affine-to-air\",\n",
    "        #    \"canonicalize\",\n",
    "        #    \"cse\"\n",
    "        #])\n",
    "        # CUSTOM: convert linalg dialect to air dialect\n",
    "        LINALG_MEMREF_TO_AIR_PIPELINE = \",\".join([\n",
    "            \"air-linalg-name\",\n",
    "            \"air-linalg-codegen{input-filter=linalg.matmul2 herd-size=8,2 l1-tile-size=16,64,32}\",\n",
    "            \"air-linalg-codegen{input-filter=linalg.generic3 herd-size=8,1 l1-tile-size=16,128,32}\",\n",
    "            \"air-rm-linalg-name\",\n",
    "            \"canonicalize\",\n",
    "            \"cse\",\n",
    "            \"affine-to-air\",\n",
    "            \"canonicalize\",\n",
    "            \"cse\"\n",
    "        ])\n",
    "        \n",
    "        pm = air.mlir.passmanager.PassManager.parse(LINALG_MEMREF_TO_AIR_PIPELINE)\n",
    "        pm.run(air_module)\n",
    "        \n",
    "        # print the air dialect mlir\n",
    "        print(air_module)\n",
    "        \n",
    "        # run aircc to build the herds\n",
    "        # the loader expects the output to be called 'torch.mlir.so'\n",
    "        aircc.run(air_module,['--shared', '-o', 'torch.mlir.so', '--sysroot=/', '-row-offset=3', '-col-offset=20', 'torch.mlir'])\n",
    "        \n",
    "        # generate a torch-mlir refbackend interface to the AIR control program so\n",
    "        # that we can reuse the refbackend's jit and object loader on the cpu.\n",
    "        with open('air_project/refback.torch.mlir') as f:\n",
    "            return_module = torch_mlir.ir.Module.parse(f.read(),imported_module.context)\n",
    "        return airbackend.refbackend.compile(return_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#map0 = affine_map<()[s0] -> (s0 * 16)>\n",
      "#map1 = affine_map<()[s0] -> (s0 * 64)>\n",
      "#map2 = affine_map<()[s0] -> (s0 * 32)>\n",
      "#map3 = affine_map<(d0, d1) -> (d0, d1)>\n",
      "module attributes {torch.debug_module_name = \"MMult_Mult\"} {\n",
      "  func @forward(%arg0: memref<128x128xi32>, %arg1: memref<128x128xi32>, %arg2: memref<128x128xi32>) -> memref<?x?xi32> {\n",
      "    %c4 = arith.constant 4 : index\n",
      "    %c2 = arith.constant 2 : index\n",
      "    %c8 = arith.constant 8 : index\n",
      "    %c0_i32 = arith.constant 0 : i32\n",
      "    %0 = memref.alloc() : memref<128x128xi32>\n",
      "    linalg.fill(%c0_i32, %0) : i32, memref<128x128xi32> \n",
      "    %1 = memref.alloc() : memref<128x128xi32>\n",
      "    linalg.copy(%0, %1) : memref<128x128xi32>, memref<128x128xi32> \n",
      "    air.launch_herd tile (%arg3, %arg4) in (%arg5=%c8, %arg6=%c2) args(%arg7=%arg1, %arg8=%arg2, %arg9=%1) : memref<128x128xi32>, memref<128x128xi32>, memref<128x128xi32> attributes {sym_name = \"herd_0\"} {\n",
      "      %c1024 = arith.constant 1024 : index\n",
      "      %c2048 = arith.constant 2048 : index\n",
      "      %c64 = arith.constant 64 : index\n",
      "      %c512 = arith.constant 512 : index\n",
      "      %c32 = arith.constant 32 : index\n",
      "      %c128 = arith.constant 128 : index\n",
      "      %c0 = arith.constant 0 : index\n",
      "      %4 = affine.apply #map0()[%arg3]\n",
      "      %5 = affine.apply #map1()[%arg4]\n",
      "      scf.for %arg10 = %c0 to %c128 step %c32 {\n",
      "        %6 = memref.alloc() : memref<16x32xi32, 2>\n",
      "        %7 = memref.alloc() : memref<32x64xi32, 2>\n",
      "        %8 = memref.alloc() : memref<16x64xi32, 2>\n",
      "        air.dma_memcpy_2d (%6, %arg7, [%c0, %c0], [%4, %arg10], %c512, %c128, %c32) {id = 1 : i32} : (memref<16x32xi32, 2>, memref<128x128xi32>, [index, index], [index, index], index, index, index) -> ()\n",
      "        air.dma_memcpy_2d (%7, %arg8, [%c0, %c0], [%arg10, %5], %c2048, %c128, %c64) {id = 2 : i32} : (memref<32x64xi32, 2>, memref<128x128xi32>, [index, index], [index, index], index, index, index) -> ()\n",
      "        air.dma_memcpy_2d (%8, %arg9, [%c0, %c0], [%4, %5], %c1024, %c128, %c64) {id = 3 : i32} : (memref<16x64xi32, 2>, memref<128x128xi32>, [index, index], [index, index], index, index, index) -> ()\n",
      "        linalg.matmul ins(%6, %7 : memref<16x32xi32, 2>, memref<32x64xi32, 2>) outs(%8 : memref<16x64xi32, 2>)\n",
      "        air.dma_memcpy_2d (%arg9, %8, [%4, %5], [%c0, %c0], %c1024, %c128, %c64) {id = 4 : i32} : (memref<128x128xi32>, memref<16x64xi32, 2>, [index, index], [index, index], index, index, index) -> ()\n",
      "        memref.dealloc %6 : memref<16x32xi32, 2>\n",
      "        memref.dealloc %7 : memref<32x64xi32, 2>\n",
      "        memref.dealloc %8 : memref<16x64xi32, 2>\n",
      "      }\n",
      "      air.herd_terminator\n",
      "    }\n",
      "    %2 = memref.alloc() : memref<128x128xi32>\n",
      "    air.launch_herd tile (%arg3, %arg4) in (%arg5=%c2, %arg6=%c4) args(%arg7=%arg0, %arg8=%1, %arg9=%2) : memref<128x128xi32>, memref<128x128xi32>, memref<128x128xi32> attributes {sym_name = \"herd_1\"} {\n",
      "      %c2048 = arith.constant 2048 : index\n",
      "      %c128 = arith.constant 128 : index\n",
      "      %c32 = arith.constant 32 : index\n",
      "      %c0 = arith.constant 0 : index\n",
      "      %4 = affine.apply #map1()[%arg3]\n",
      "      %5 = affine.apply #map2()[%arg4]\n",
      "      %6 = memref.alloc() : memref<64x32xi32, 2>\n",
      "      %7 = memref.alloc() : memref<64x32xi32, 2>\n",
      "      %8 = memref.alloc() : memref<64x32xi32, 2>\n",
      "      air.dma_memcpy_2d (%6, %arg7, [%c0, %c0], [%4, %5], %c2048, %c128, %c32) {id = 5 : i32} : (memref<64x32xi32, 2>, memref<128x128xi32>, [index, index], [index, index], index, index, index) -> ()\n",
      "      air.dma_memcpy_2d (%7, %arg8, [%c0, %c0], [%4, %5], %c2048, %c128, %c32) {id = 6 : i32} : (memref<64x32xi32, 2>, memref<128x128xi32>, [index, index], [index, index], index, index, index) -> ()\n",
      "      air.dma_memcpy_2d (%8, %arg9, [%c0, %c0], [%4, %5], %c2048, %c128, %c32) {id = 7 : i32} : (memref<64x32xi32, 2>, memref<128x128xi32>, [index, index], [index, index], index, index, index) -> ()\n",
      "      linalg.generic {indexing_maps = [#map3, #map3, #map3], iterator_types = [\"parallel\", \"parallel\"]} ins(%6, %7 : memref<64x32xi32, 2>, memref<64x32xi32, 2>) outs(%8 : memref<64x32xi32, 2>) {\n",
      "      ^bb0(%arg10: i32, %arg11: i32, %arg12: i32):\n",
      "        %9 = arith.muli %arg10, %arg11 : i32\n",
      "        linalg.yield %9 : i32\n",
      "      }\n",
      "      air.dma_memcpy_2d (%arg9, %8, [%4, %5], [%c0, %c0], %c2048, %c128, %c32) {id = 8 : i32} : (memref<128x128xi32>, memref<64x32xi32, 2>, [index, index], [index, index], index, index, index) -> ()\n",
      "      memref.dealloc %6 : memref<64x32xi32, 2>\n",
      "      memref.dealloc %7 : memref<64x32xi32, 2>\n",
      "      memref.dealloc %8 : memref<64x32xi32, 2>\n",
      "      air.herd_terminator\n",
      "    }\n",
      "    %3 = memref.cast %2 : memref<128x128xi32> to memref<?x?xi32>\n",
      "    return %3 : memref<?x?xi32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compiled = compile(mb.module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "metal: info:      Registered shmem provider linux_shm.\n",
      "metal: info:      Registered shmem provider ion.reserved.\n",
      "metal: info:      Registered shmem provider ion.ion_system_contig_heap.\n",
      "metal: info:      Registered shmem provider ion.ion_system_heap.\n",
      "metal: info:      device xilinx-aiengine in use by driver uio_dmem_genirq\n",
      "metal: warning:   metal_linux_irq_handling: Failed to set scheduler: Unknown error -1.\n",
      "metal: info:      metal_uio_dev_open: No IRQ for device f70a0000.aie-npi.\n"
     ]
    }
   ],
   "source": [
    "jit_module = airbackend.load(compiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "tensor([[69, 30, 85,  ..., 26, 59, 90],\n",
      "        [95, 55, 65,  ..., 49, 12, 96],\n",
      "        [40, 14, 94,  ..., 80, 89, 25],\n",
      "        ...,\n",
      "        [54,  1, 82,  ..., 94,  5,  3],\n",
      "        [83, 44, 86,  ..., 51,  9, 26],\n",
      "        [59, 62, 60,  ..., 77, 89, 25]], dtype=torch.int32)\n",
      "tensor([[13, 75, 11,  ..., 30, 85, 58],\n",
      "        [26, 83, 40,  ..., 67, 31, 67],\n",
      "        [30, 42, 71,  ...,  3, 10, 19],\n",
      "        ...,\n",
      "        [82, 59,  4,  ..., 37, 42, 52],\n",
      "        [60, 68, 12,  ...,  6, 35, 99],\n",
      "        [ 9, 10, 27,  ...,  1, 45, 41]], dtype=torch.int32)\n",
      "tensor([[23, 52, 71,  ..., 24, 20, 92],\n",
      "        [59, 13, 62,  ..., 57, 58, 86],\n",
      "        [71, 27, 78,  ..., 35, 31, 84],\n",
      "        ...,\n",
      "        [59, 66, 48,  ..., 68, 99, 93],\n",
      "        [32,  0, 35,  ..., 96,  5, 69],\n",
      "        [63, 16, 85,  ...,  3, 52,  5]], dtype=torch.int32)\n",
      "output:\n",
      "tensor([[21063975,  8830350, 26757575,  ...,  8170474, 17032002, 26694990],\n",
      "        [30323240, 19040615, 24089585,  ..., 18361966,  4016112, 32654880],\n",
      "        [11527480,  4216072, 29995964,  ..., 24195600, 26053415,  7527475],\n",
      "        ...,\n",
      "        [14956596,   290496, 25749968,  ..., 27938868,  1388610,   897375],\n",
      "        [23850299, 13999348, 28384644,  ..., 16725654,  2852253,  7867028],\n",
      "        [14636897, 17190926, 18383160,  ..., 22995665, 22580101,  6981700]],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(100, SIZE, dtype=torch.int32)\n",
    "b = torch.randint(100, SIZE, dtype=torch.int32)\n",
    "c = torch.randint(100, SIZE, dtype=torch.int32)\n",
    "\n",
    "# run the model on the device\n",
    "o = jit_module.forward(a.numpy(),b.numpy(),c.numpy())\n",
    "\n",
    "# print the results\n",
    "d = torch.tensor(o)    \n",
    "print(f\"input:\\n{a}\\n{b}\\n{c}\\noutput:\\n{d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS!\n"
     ]
    }
   ],
   "source": [
    "# check the results\n",
    "if torch.equal(a*torch.mm(b,c),d):\n",
    "    print(\"PASS!\")\n",
    "else:\n",
    "    print(\"failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "air-mlir",
   "language": "python",
   "name": "air-mlir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
